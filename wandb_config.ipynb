{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"DNN Localization\",  # Set your project name here\n",
    "    name='ciao',\n",
    "    config={\n",
    "        'epochs': 10,\n",
    "        'batch_size': 250,\n",
    "        'lr': 1e-3,\n",
    "    }   # Pass all the arguments to the config section of WandB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--id ID] [--epochs EPOCHS] [--N N]\n",
      "                             [--N_RF N_RF] [--type TYPE]\n",
      "                             [--generate_dataset GENERATE_DATASET]\n",
      "                             [--train TRAIN] [--lr LR]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--train_split TRAIN_SPLIT] [--logdir LOGDIR]\n",
      "                             [--scheduler SCHEDULER]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\Mattia\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-16068tHiS8Gumpubz.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mattia\\anaconda3\\envs\\beamwise\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Mattia Fabiani\n",
    "Update: This version of the main allows the grouping of the results by SNR values.\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import os\n",
    "# from torch.utils.data import DataLoader\n",
    "from dnn_model import DNN_model\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from network_functions import train_loop, eval_loop, test_loop, evaluate_predictions\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--id', type=str,                   default='', help='Unique experiment identifier')\n",
    "parser.add_argument('--epochs', type=int,               default=50, help='Number of train epochs.')\n",
    "parser.add_argument('--N', type=int,                    default=128, help='Number of antennas.')\n",
    "parser.add_argument('--N_RF', type=int,                 default=16, help='Number of RF chains.')\n",
    "parser.add_argument('--type', type=str,                 default='fully-connected', help='RF chains to antenna connection')\n",
    "parser.add_argument('--generate_dataset', type=int,     default=0, help='Generate Dataset.')\n",
    "parser.add_argument('--train', type=int,                default=1, help='Train the DNN model.')\n",
    "# parser.add_argument('--drop', type=float,               default=0., help='Dropout.')\n",
    "parser.add_argument('--lr', type=float,                 default=1e-3, help='Learning rate.')\n",
    "parser.add_argument('--batch_size', type=int,           default=250, help='Batch size')\n",
    "parser.add_argument('--train_split', type=float,        default=0.8, help='Train split')\n",
    "parser.add_argument('--logdir', type=str,               default='saved_models', help='Directory to log data to')\n",
    "parser.add_argument('--scheduler', type=int,            default=0, help='use scheduler to control the learning rate')\n",
    "args = parser.parse_args()\n",
    "foldername = args.type + '_' +'epochs'+str(args.epochs)+'_batch'+str(args.batch_size)+'_lr'+str(args.lr)+'_'+str(args.N_RF)+'RF_'+str(args.N)+'N'\n",
    "args.logdir = os.path.join(args.logdir, foldername + '_' + args.id)\n",
    "\n",
    "#---------- SIMULATION PARAMETERS -----------\n",
    "f0 = 25e9                   # carrier frequency\n",
    "k = 2*np.pi / (3e8 / f0)    # wave number\n",
    "d = 3e8/f0 / 2              # antenna spacing\n",
    "N = args.N                     # antennas\n",
    "N_RF = args.N_RF                   # RF chains\n",
    "SNR_dB = list(range(0,25,5))\n",
    "SNR = [10 ** (SNR / 10) for SNR in SNR_dB]\n",
    "\n",
    "range_limits = [1, 20]      # near-field range limits [m]\n",
    "\n",
    "dataset_size = 20000        # number of signals per SNR (10k * 5 = 50k samples in ottal)\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "train_val_split = args.train_split\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "dataset_root = 'dataset/'\n",
    "dataset_path = os.path.join(dataset_root,'dataset.npy')\n",
    "models = [DNN_model]        # create a list of models if more than one are developed\n",
    "rng_seed = 42\n",
    "#------------------------------------------\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"my_project_name\",\n",
    "    name=args.type + str(args.N_RF) + str(args.N),\n",
    "    config=args\n",
    ")\n",
    "config = wandb.config\n",
    "epochs = config.epochs\n",
    "batch_size = config.batch_size\n",
    "lr = config.lr\n",
    "\n",
    "#%% Data Prep\n",
    "\n",
    "if args.generate_dataset:\n",
    "    def CN_realization(mean, std_dev, size=1):\n",
    "        return np.random.normal(mean, std_dev, size) + 1j * np.random.normal(mean, std_dev, size)\n",
    "\n",
    "\n",
    "    delta = lambda n: (2*n - N + 1)/2\n",
    "\n",
    "    # near-field array response vector (parabolic wavefront approximation)\n",
    "    a = lambda theta, r: np.array([np.exp(-1j*k*(np.sqrt(r**2 + delta(n)**2*d**2 - 2*r*theta*delta(n)*d) - r)) for n in range(N)]).T\n",
    "\n",
    "    dataset = dict()\n",
    "\n",
    "\n",
    "    np.random.seed(rng_seed)\n",
    "    ii = 0\n",
    "    for i in range(dataset_size):\n",
    "        for snr in SNR:\n",
    "            sigma_n = 1 / np.sqrt(snr)\n",
    "            s = CN_realization(mean=0, std_dev=1)\n",
    "            n = CN_realization(mean=0, std_dev=sigma_n, size=N)\n",
    "            \n",
    "            r = np.random.uniform(range_limits[0], range_limits[1])\n",
    "            r_norm = 2 * (r - range_limits[0]) / (range_limits[1] - range_limits[0]) - 1\n",
    "            theta = np.random.uniform(-1,1)\n",
    "            \n",
    "            # uplink received signal\n",
    "            y_ = a(theta,r) * s + n\n",
    "            y = np.concatenate((y_.real, y_.imag))\n",
    "            \n",
    "            datapoint = {\n",
    "                'SNR': snr,\n",
    "                'X': y,\n",
    "                'y': np.array([theta, r_norm])\n",
    "            }\n",
    "            \n",
    "            dataset[ii] = datapoint\n",
    "            ii += 1\n",
    "\n",
    "    if not os.path.exists(dataset_root):\n",
    "        os.makedirs(dataset_root)\n",
    "\n",
    "    np.save(dataset_path,dataset,allow_pickle=True)\n",
    "    print(f\"Dataset saved to {dataset_path}\")\n",
    "    print(len(dataset))\n",
    "\n",
    "# Load Data\n",
    "dataset = np.load(dataset_path,allow_pickle=True).item()\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "train_dim = int(train_val_split*dataset_size*len(SNR))\n",
    "val_dim = int(val_split*dataset_size*len(SNR))\n",
    "test_dim = int(test_split*dataset_size*len(SNR))\n",
    "\n",
    "X_train, y_train = np.array([dataset[i]['X'] for i in range(train_dim)]), np.array([dataset[i]['y'] for i in range(train_dim)])\n",
    "X_val, y_val = np.array([dataset[i]['X'] for i in range(train_dim,train_dim+val_dim)]), np.array([dataset[i]['y'] for i in range(train_dim,train_dim+val_dim)])\n",
    "X_test, y_test = np.array([dataset[i]['X'] for i in range(train_dim+val_dim,train_dim+val_dim + test_dim)]), np.array([dataset[i]['y'] for i in range(train_dim+val_dim,train_dim+val_dim + test_dim)])\n",
    "# X_test, y_test = np.array([dataset[i] for i in range(train_dim+val_dim,train_dim+val_dim + test_dim)]), np.array([dataset[i]['y'] for i in range(train_dim+val_dim,train_dim+val_dim + test_dim)])\n",
    "SNR_train = torch.tensor([np.where(dataset[i]['SNR']==np.array(SNR))[0] for i in range(train_dim)]).squeeze()\n",
    "SNR_val = torch.tensor([np.where(dataset[i]['SNR']==np.array(SNR))[0] for i in range(val_dim)]).squeeze()\n",
    "SNR_test = torch.tensor([np.where(dataset[i]['SNR']==np.array(SNR))[0] for i in range(test_dim)]).squeeze()\n",
    "\n",
    "print('train set: ',len(X_train))\n",
    "print('val set: ',len(X_val))\n",
    "print('test set: ',len(X_test))\n",
    "# X_test = np.array([dataset[i]['X'] for i in range(train_dim+val_dim,train_dim+val_dim + test_dim)])\n",
    "\n",
    "# xy_test = [dataset[i] for i in range(train_dim+val_dim,train_dim+val_dim + test_dim)]\n",
    "# df_test = pd.DataFrame([{key: value for key, value in record.items()} for record in xy_test])\n",
    "# y_test = df_test[['y','SNR']].to_numpy()\n",
    "# print(np.array(df_test[['X']].values).squeeze().shape)\n",
    "# X_test = torch.tensor(X_test).float()\n",
    "# exit()\n",
    "\n",
    "# PyTorch Tensors\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# PyTorch GPU/CPU selection\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #%% Training\n",
    "if args.train:\n",
    "    data_type = 0\n",
    "    # Model Save Folder\n",
    "    model_directory = args.logdir\n",
    "    if not os.path.exists(model_directory):\n",
    "        os.makedirs(model_directory)\n",
    "    print('Saving the models to %s' % model_directory)\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(rng_seed)\n",
    "    np.random.seed(rng_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # PyTorch GPU/CPU selection\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Neural Network\n",
    "    model = models[data_type](N,N_RF)\n",
    "\n",
    "    # Applying the constraints to the first layer only (precoder power constraint)\n",
    "    # constraints=weightConstraint(N,N_RF)\n",
    "    # model._modules['fc1'].apply(constraints)\n",
    "    trainable_params = list(model.parameters())\n",
    "    num_trainable_params = sum(p.numel() for p in trainable_params if p.requires_grad)\n",
    "    print(\"Number of trainable parameters:\", num_trainable_params)\n",
    "\n",
    "    # Training Settings\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.MSELoss() # Training Criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) # Optimizer\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    if args.scheduler:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=20, threshold=0.0001)\n",
    "\n",
    "    train_loss = np.zeros((epochs))\n",
    "    train_rmse_r = []\n",
    "    train_rmse_theta = []\n",
    "    train_rmse_pos = []\n",
    "    val_rmse_r = []\n",
    "    val_rmse_theta = []\n",
    "    val_rmse_pos = []\n",
    "    min_val_loss = 100\n",
    "    train_acc = np.zeros((epochs))\n",
    "    val_loss = np.zeros((epochs))\n",
    "    val_acc = np.zeros((epochs))\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %i/%i:'%(epoch+1, epochs), flush=True)\n",
    "        print('lr = ',optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        train_loss[epoch], RMSE_train = train_loop(X_train, y_train, SNR_train, model, optimizer, criterion, device,N,N_RF,range_limits,type=args.type, batch_size=batch_size)\n",
    "        val_loss[epoch], RMSE_val = eval_loop(X_val, y_val, SNR_val, model, criterion, device,N,N_RF,range_limits,type=args.type, batch_size=batch_size)\n",
    "        data = {\n",
    "            'SNR [dB]': SNR_dB,\n",
    "            'Train (r)': RMSE_train['r'],\n",
    "            'Val (r)': RMSE_val['r'],\n",
    "            'Train (theta)': RMSE_train['theta'],\n",
    "            'Val (theta)': RMSE_val['theta'],\n",
    "            'Train (pos)': RMSE_train['pos'],\n",
    "            'Val (pos)': RMSE_val['pos'],\n",
    "        }\n",
    "\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss[epoch], \"val_loss\": val_loss[epoch]})\n",
    "        wandb.watch(model)\n",
    "\n",
    "        df = pd.DataFrame(data,index=SNR_dB)\n",
    "        df.index.name = 'SNR [dB]'\n",
    "        print(df)\n",
    "        print(f'Train loss: {train_loss[epoch]:.4f}, Val loss: {val_loss[epoch]:.4f}')\n",
    "        if val_loss[epoch] < min_val_loss:\n",
    "            print('saving results...')\n",
    "            print(f'Val loss improved: {min_val_loss:.3f} -> {val_loss[epoch]:.3f}')\n",
    "            min_val_loss = val_loss[epoch]\n",
    "            print('Saving model..\\n')\n",
    "            torch.save(model.state_dict(), os.path.join(model_directory, 'model_best.pth'))\n",
    "            df.to_csv(os.path.join(args.logdir,'best_rmse_epoch.csv'),index=False)\n",
    "            wandb.save(os.path.join(model_directory, 'model_best.pth'))\n",
    "\n",
    "        train_rmse_r.append(RMSE_train['r'])\n",
    "        train_rmse_theta.append(RMSE_train['theta'])\n",
    "        train_rmse_pos.append(RMSE_train['pos'])\n",
    "        val_rmse_r.append(RMSE_val['r'])\n",
    "        val_rmse_theta.append(RMSE_val['theta'])\n",
    "        val_rmse_pos.append(RMSE_val['pos'])\n",
    " \n",
    "        # Save the best model\n",
    "        # if val_loss[epoch] <= np.min(val_loss[:epoch] if epoch>0 else val_loss[epoch]):\n",
    "        #     print('Saving model..')\n",
    "        #     torch.save(model.state_dict(), os.path.join(model_directory, 'model_best.pth'))\n",
    "        #     # torch.save(model.state_dict(), 'saved_models/type0_batchsize5_rng42_epoch500_v11/model_best.pth')\n",
    "        if args.scheduler:\n",
    "            scheduler.step(val_loss[-1])\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(model_directory, 'model_final.pth'))\n",
    "    np.save(os.path.join(model_directory, 'train_loss.npy'),train_loss)\n",
    "    np.save(os.path.join(model_directory, 'val_loss.npy'),val_loss)\n",
    "    np.save(os.path.join(model_directory, 'train_rmse_r.npy'),train_rmse_r)\n",
    "    np.save(os.path.join(model_directory, 'train_rmse_theta.npy'),train_rmse_theta)\n",
    "    np.save(os.path.join(model_directory, 'train_rmse_pos.npy'),train_rmse_pos)\n",
    "    np.save(os.path.join(model_directory, 'val_rmse_r.npy'),val_rmse_r)\n",
    "    np.save(os.path.join(model_directory, 'val_rmse_theta.npy'),val_rmse_theta)\n",
    "    np.save(os.path.join(model_directory, 'val_rmse_pos.npy'),val_rmse_pos)\n",
    "    wandb.log({\"final_train_loss\": train_loss[-1], \"final_val_loss\": val_loss[-1]})\n",
    "    wandb.log({\"train_rmse_r\": np.mean(train_rmse_r), \"val_rmse_r\": np.mean(val_rmse_r)})\n",
    "    wandb.log({\"train_rmse_theta\": np.mean(train_rmse_theta), \"val_rmse_theta\": np.mean(val_rmse_theta)})\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_loss,'k',label='train loss')\n",
    "    plt.plot(val_loss,'r',label='val loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_rmse_r,'k',label='train rmse (r)')\n",
    "    plt.plot(val_rmse_r,'r',label='val rmse (r)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE (m)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_rmse_theta,'k',label='train rmse (theta)')\n",
    "    plt.plot(val_rmse_theta,'r',label='val rmse (theta)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE (deg)')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_rmse_pos,'k',label='train rmse (pos)')\n",
    "    plt.plot(val_rmse_pos,'r',label='val rmse (pos)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE (m)')\n",
    "    plt.show()\n",
    "\n",
    "    print('Finished Training')\n",
    "else:\n",
    "    #%% Test\n",
    "\n",
    "    print('Testing..')\n",
    "    # topk=5\n",
    "    model_directory = 'saved_models_dataset50000/type0_batchsize64_rng42_epoch50_v4_16RF'\n",
    "    model_PATH = model_directory + '/model_best.pth'\n",
    "    model = models[0](N,N_RF)\n",
    "    model.load_state_dict(torch.load(model_PATH))\n",
    "    \n",
    "    criterion = torch.nn.MSELoss() # Training Criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) # Optimizer\n",
    "\n",
    "    val_loss, RMSE_val = eval_loop(X_val, y_val, SNR_val, model, criterion, device,N,N_RF,range_limits, batch_size=batch_size)\n",
    "    \n",
    "    data = {\n",
    "            'Val (r)': RMSE_val['r'],\n",
    "            'Val (theta)': RMSE_val['theta'],\n",
    "            'Val (pos)': RMSE_val['pos']\n",
    "        }\n",
    "    df = pd.DataFrame(data,index=SNR_dB)\n",
    "    df.index.name = 'SNR [dB]'\n",
    "    print(df)\n",
    "    # network_time_per_sample = test_loop(X_test, y_test, SNR_test, model, device,N,N_RF,range_limits, model_path=os.path.join(model_directory, 'model_best.pth')) # Best model\n",
    "    # print(network_time_per_sample)\n",
    "    \n",
    "    # topk_acc_best, beam_dist_best = evaluate_predictions(y, y_hat, k=topk)\n",
    "    # print('Best model:')\n",
    "    # print('Top-k Accuracy: ' + '-'.join(['%.2f' for i in range(topk)]) % tuple(topk_acc_best*100))\n",
    "    # print('Beam distance: %.2f' % beam_dist_best)\n",
    "\n",
    "\n",
    "    # y_final, y_hat_final, network_time_per_sample_final = test_loop(X_test, y_test, model, device, model_path=os.path.join(model_directory, 'model_final.pth')) # Last Epoch\n",
    "    # topk_acc_final, beam_dist_final = evaluate_predictions(y_final, y_hat_final, k=topk)\n",
    "    # print('Final model:')\n",
    "    # print('Top-k Accuracy: ' + '-'.join(['%.2f' for i in range(topk)]) % tuple(topk_acc_final*100))\n",
    "    # print('Beam distance: %.2f' % beam_dist_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beamwise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
